{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ubbf6tzg5GF",
        "outputId": "20c55dfc-ce32-41d1-8ab6-074bb6598a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_zillow(location):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
        "                      '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    base_url = f\"https://www.zillow.com/{location}/\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            listings = soup.find_all('article', class_='list-card')\n",
        "\n",
        "            with open('zillow_properties.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(['Title', 'Price', 'URL'])\n",
        "\n",
        "                for listing in listings:\n",
        "                    title = listing.find('a', class_='list-card-link').text\n",
        "                    price = listing.find('div', class_='list-card-price').text\n",
        "                    property_url = listing.find('a', class_='list-card-link')['href']\n",
        "\n",
        "                    writer.writerow([title, price, property_url])\n",
        "\n",
        "            print(\"Scraping successful! Data saved to 'zillow_properties.csv'\")\n",
        "        else:\n",
        "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    location = \"san-francisco-ca\"  # Replace with your desired location (e.g., city or zip code)\n",
        "    scrape_zillow(location)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKnSvE-qNf0z",
        "outputId": "4a5b4022-e595-458c-a5ed-3a2f4e52659d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch the page. Status code: 403\n"
          ]
        }
      ]
    }
  ]
}